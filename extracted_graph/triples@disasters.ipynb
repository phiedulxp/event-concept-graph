{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "class StanfordCoreNLP:\n",
    "    '''\n",
    "    Wrapper for Starford Corenlp Restful API\n",
    "    annotators:\"truecase,tokenize,ssplit,pos,lemma,ner,regexner,parse,depparse,openie,coref,kbp,sentiment\"\n",
    "    nlp = StanfordCoreNLP()\n",
    "    output = nlp.annotate(text, properties={ 'annotators':'kbp','outputFormat': 'json',})\n",
    "    output.keys()\n",
    "    dict_keys(['sentences', 'corefs'])\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, host='127.0.0.1', port='9000'):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "\n",
    "    def annotate(self, data, properties=None, lang='en'):\n",
    "        self.server_url = 'http://'+self.host+':'+self.port\n",
    "        properties['outputFormat'] = 'json'\n",
    "        try:\n",
    "            res = requests.post(self.server_url,\n",
    "                                params={'properties': str(properties),\n",
    "                                        'pipelineLanguage':lang},\n",
    "                                data=data, \n",
    "                                headers={'Connection': 'close'})\n",
    "            return res.json()\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snlp = StanfordCoreNLP(host='10.0.1.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "wv = KeyedVectors.load_word2vec_format('glove.6B.300d.bin', binary=True)\n",
    "annoy_index = AnnoyIndexer()\n",
    "annoy_index.load('glove.6B.300d.index')\n",
    "annoy_index.model = wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_disasters_and_accdients = dict(wv.most_similar(positive=['disaster','accident',], topn=25000, indexer=annoy_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teutoburg', 0.2927718758583069),\n",
       " ('scranton', 0.2927703857421875),\n",
       " ('steffi', 0.29277002811431885),\n",
       " ('constructive', 0.2927693724632263),\n",
       " ('so-called', 0.2927691340446472),\n",
       " ('floodlight', 0.2927657961845398),\n",
       " ('worldnews@ap.org', 0.2927653193473816),\n",
       " ('g/mol', 0.2927647829055786),\n",
       " ('binga', 0.29276204109191895),\n",
       " ('single-goal', 0.2927611470222473)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_disasters_and_accdients.items())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_load(text):\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_event = db.read_text('./news_event.jsonl').map(safe_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def category_filter(sample):\n",
    "    if not sample:\n",
    "        return False\n",
    "    return sample['event']['category'].startswith('disaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_event_disasters_and_accdients = news_event.filter(category_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'disaster and accident',\n",
       " 'date': '2010-07-01',\n",
       " 'description': 'Hurricane Alex, the first hurricane of the 2010 Atlantic hurricane season, makes landfall in northeastern Mexico as a Category\\xa02 hurricane on the Saffir-Simpson Hurricane Scale with winds of 105\\xa0mph (165\\xa0km/h), and causes tornadoes that force people into shelters in southern Texas.',\n",
       " 'id': '59fbc9fa60b18848c5a4ce61',\n",
       " 'media': ['National Hurricane Center', 'The Australian', 'Aljazeera'],\n",
       " 'title': ''}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_event_disasters_and_accdients.take(1)[0]['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = news_event_disasters_and_accdients.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['event', 'tweets'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'disaster and accident',\n",
       " 'date': '2010-07-01',\n",
       " 'description': 'Hurricane Alex, the first hurricane of the 2010 Atlantic hurricane season, makes landfall in northeastern Mexico as a Category\\xa02 hurricane on the Saffir-Simpson Hurricane Scale with winds of 105\\xa0mph (165\\xa0km/h), and causes tornadoes that force people into shelters in southern Texas.',\n",
       " 'id': '59fbc9fa60b18848c5a4ce61',\n",
       " 'media': ['National Hurricane Center', 'The Australian', 'Aljazeera'],\n",
       " 'title': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boe_cosine': 0.9158096833057836,\n",
       " 'created_at': '2010-06-29 13:22:38',\n",
       " 'data_name': 'Jacqui Jeras',\n",
       " 'favorites': 0,\n",
       " 'has_url': False,\n",
       " 'hashtags': 0,\n",
       " 'id': '17331748501',\n",
       " 'is_reply': False,\n",
       " 'is_retweet': False,\n",
       " 'lang': 'en',\n",
       " 'mentions': 0,\n",
       " 'replies': 0,\n",
       " 'retweets': 1,\n",
       " 'screen_name': 'JacquiJerasTV',\n",
       " 'text': 'Alex nearly a hurricane; winds 70mph. Texas will start to feel TS force winds tomorrow. Heavy rains expected 5-10\". Landfall near border.',\n",
       " 'userbadges': 'Verifizierter Account'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['tweets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_event = sample['event']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_tweet = sample['tweets'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo = 'I like and hate him.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = snlp.annotate(demo_event, properties={ 'annotators':'ner,depparse','outputFormat': 'json',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentences'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out['corefs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'basicDependencies', 'enhancedDependencies', 'enhancedPlusPlusDependencies', 'entitymentions', 'tokens'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['sentences'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['sentences'][0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_dict = {}\n",
    "[tokens_dict.update({t['index']:{'lemma':t['lemma'].lower(),'word':t['word'],'pos':t['pos']}}) for t in out['sentences'][0]['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(tokens_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity = [{'text':e['text'].lower(),'ner':e['ner'].lower(),'tokenBegin':e['tokenBegin']+1,'tokenEnd':e['tokenEnd'],} for e in out['sentences'][0]['entitymentions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NER_ = ['person','ordinal','number','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_dict = {}\n",
    "span_entity = []\n",
    "entity_triples = []\n",
    "for e in entity:\n",
    "    #entity_dict.update({e['tokenEnd']:e['ner'] if e['ner'] in NER_ else \\\n",
    "    #                    [e['ner'],e['text'] if e['tokenEnd'] == e['tokenBegin']\\\n",
    "    #                     else ''.join([tokens_dict[i]['lemma'] for i in range(e['tokenBegin'],e['tokenEnd']+1)])]})\n",
    "    entity_dict.update({e['tokenEnd']:e['ner'] if e['ner'] in NER_ else e['text']})#[e['ner'],e['text']]})\n",
    "    if e['ner'] not in NER_:\n",
    "        entity_triples.append(('/c/en/'+e['text'],'/r/tweet/IsA','/c/en/'+e['ner']))\n",
    "    if e['tokenEnd'] != e['tokenBegin']:\n",
    "        span_entity.append((e['tokenBegin'],e['tokenEnd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'hurricane',\n",
       " 2: 'person',\n",
       " 5: 'ordinal',\n",
       " 6: 'hurricane',\n",
       " 9: 'date',\n",
       " 10: 'atlantic',\n",
       " 11: 'hurricane',\n",
       " 18: 'mexico',\n",
       " 22: 'number',\n",
       " 23: 'hurricane',\n",
       " 27: 'hurricane',\n",
       " 32: 'number',\n",
       " 35: 'number',\n",
       " 49: 'texas'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/atlantic', '/r/tweet/IsA', '/c/en/location'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/mexico', '/r/tweet/IsA', '/c/en/country'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/texas', '/r/tweet/IsA', '/c/en/state_or_province')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_triples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out['sentences'][0]['basicDependencies']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out['sentences'][0]['enhancedDependencies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep = out['sentences'][0]['enhancedPlusPlusDependencies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEP_ = ['amod','punct','case','det','ROOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "list_stopWords=list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep = [{'from':d['governor'],'dep':d['dep'],'to':d['dependent']} for d in dep if d['dep'] not in DEP_ \\\n",
    "       and (d['dependent'],d['governor']) not in span_entity and d['dependentGloss'] not in list_stopWords\\\n",
    "      and d['governorGloss'] not in list_stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_to_str(index):\n",
    "    return entity_dict[index] if index in entity_dict.keys() else tokens_dict[index]['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep_triples =[('/c/en/'+dep_to_str(d['to']),'/r/tweet/dep/'+d['dep'],'/c/en/'+dep_to_str(d['from'])) for d in dep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fact_dict = defaultdict(dict)\n",
    "for d in dep:\n",
    "    if d['dep'].startswith('nsubj') or d['dep'].startswith('dobj'):\n",
    "            fact_dict[d['from']].update({d['dep']:d['to']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {14: {'dobj': 15, 'nsubj': 2},\n",
       "             40: {'dobj': 41, 'nsubj': 2},\n",
       "             43: {'dobj': 44, 'nsubj': 41}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fact_triples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,v in fact_dict.items():\n",
    "    if 'nsubj' in v.keys() and 'dobj' in v.keys():\n",
    "        fact_triples.append(('/c/en/'+dep_to_str(v['nsubj']),'/r/tweet/open/'+dep_to_str(k),'/c/en/'+dep_to_str(v['dobj'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/c/en/person', '/r/tweet/open/make', '/c/en/landfall'),\n",
       " ('/c/en/person', '/r/tweet/open/cause', '/c/en/tornado'),\n",
       " ('/c/en/tornado', '/r/tweet/open/force', '/c/en/people')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triples = entity_triples + dep_triples + fact_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/atlantic', '/r/tweet/IsA', '/c/en/location'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/mexico', '/r/tweet/IsA', '/c/en/country'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/hurricane', '/r/tweet/IsA', '/c/en/cause_of_death'),\n",
       " ('/c/en/texas', '/r/tweet/IsA', '/c/en/state_or_province'),\n",
       " ('/c/en/hurricane', '/r/tweet/dep/compound', '/c/en/person'),\n",
       " ('/c/en/person', '/r/tweet/dep/nsubj', '/c/en/make'),\n",
       " ('/c/en/person', '/r/tweet/dep/nsubj', '/c/en/cause'),\n",
       " ('/c/en/hurricane', '/r/tweet/dep/appos', '/c/en/person'),\n",
       " ('/c/en/date', '/r/tweet/dep/nummod', '/c/en/season'),\n",
       " ('/c/en/atlantic', '/r/tweet/dep/compound', '/c/en/season'),\n",
       " ('/c/en/hurricane', '/r/tweet/dep/compound', '/c/en/season'),\n",
       " ('/c/en/season', '/r/tweet/dep/nmod:of', '/c/en/hurricane'),\n",
       " ('/c/en/landfall', '/r/tweet/dep/dobj', '/c/en/make'),\n",
       " ('/c/en/mexico', '/r/tweet/dep/nmod:in', '/c/en/landfall'),\n",
       " ('/c/en/category', '/r/tweet/dep/compound', '/c/en/hurricane'),\n",
       " ('/c/en/number', '/r/tweet/dep/nummod', '/c/en/hurricane'),\n",
       " ('/c/en/hurricane', '/r/tweet/dep/nmod:as', '/c/en/make'),\n",
       " ('/c/en/saffir-simpson', '/r/tweet/dep/compound', '/c/en/scale'),\n",
       " ('/c/en/hurricane', '/r/tweet/dep/compound', '/c/en/scale'),\n",
       " ('/c/en/scale', '/r/tweet/dep/nmod:on', '/c/en/hurricane'),\n",
       " ('/c/en/wind', '/r/tweet/dep/nmod:with', '/c/en/hurricane'),\n",
       " ('/c/en/number', '/r/tweet/dep/nummod', '/c/en/mph'),\n",
       " ('/c/en/mph', '/r/tweet/dep/nmod:of', '/c/en/wind'),\n",
       " ('/c/en/number', '/r/tweet/dep/nummod', '/c/en/km/h'),\n",
       " ('/c/en/km/h', '/r/tweet/dep/appos', '/c/en/mph'),\n",
       " ('/c/en/cause', '/r/tweet/dep/conj:and', '/c/en/make'),\n",
       " ('/c/en/tornado', '/r/tweet/dep/dobj', '/c/en/cause'),\n",
       " ('/c/en/tornado', '/r/tweet/dep/nsubj', '/c/en/force'),\n",
       " ('/c/en/force', '/r/tweet/dep/acl:relcl', '/c/en/tornado'),\n",
       " ('/c/en/people', '/r/tweet/dep/dobj', '/c/en/force'),\n",
       " ('/c/en/shelter', '/r/tweet/dep/nmod:into', '/c/en/force'),\n",
       " ('/c/en/texas', '/r/tweet/dep/nmod:in', '/c/en/shelter'),\n",
       " ('/c/en/person', '/r/tweet/open/make', '/c/en/landfall'),\n",
       " ('/c/en/person', '/r/tweet/open/cause', '/c/en/tornado'),\n",
       " ('/c/en/tornado', '/r/tweet/open/force', '/c/en/people')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "list_stopWords=list(set(stopwords.words('english')))\n",
    "NER_ = ['person','ordinal','number','date']\n",
    "DEP_ = ['amod','punct','case','det','ROOT','dep']\n",
    "def dep_to_str(tokens_dict,entity_dict,index):\n",
    "    return entity_dict[index] if index in entity_dict.keys() else tokens_dict[index]['lemma']\n",
    "\n",
    "def annotate_sentences(text):\n",
    "    out = snlp.annotate(text, properties={ 'annotators':'ner,depparse','outputFormat': 'json'})\n",
    "    #import pdb;pdb.set_trace()\n",
    "    triples = []\n",
    "    for sentence in out['sentences']:\n",
    "        #print(sentence['index'])\n",
    "        triples.extend(get_triples(sentence))\n",
    "    return triples\n",
    "    \n",
    "def get_triples(sentence):\n",
    "    # tokens \n",
    "    tokens_dict = {}\n",
    "    [tokens_dict.update({t['index']:{'lemma':t['lemma'].lower(),'word':t['word'],'pos':t['pos']}}) for t in sentence['tokens']]\n",
    "    # entity \n",
    "    entity = [{'text':e['text'].lower(),'ner':e['ner'].lower(),'tokenBegin':e['tokenBegin']+1,'tokenEnd':e['tokenEnd'],}\\\n",
    "              for e in sentence['entitymentions']]\n",
    "    entity_dict = {}\n",
    "    span_entity = [] # combine in dep\n",
    "    entity_triples = []\n",
    "    for e in entity:\n",
    "        entity_dict.update({e['tokenEnd']:e['ner'] if e['ner'] in NER_ else e['text']})#[e['ner'],e['text']]})\n",
    "        if e['ner'] not in NER_:\n",
    "            entity_triples.append(('/c/en/'+e['text'],'/r/tweet/IsA','/c/en/'+e['ner']))\n",
    "            #print('/c/en/'+e['text'],'/r/tweet/IsA','/c/en/'+e['ner'])\n",
    "        if e['tokenEnd'] != e['tokenBegin']:\n",
    "            span_entity.append((e['tokenBegin'],e['tokenEnd']))\n",
    "    # dep triples\n",
    "    # vocab_disasters_and_accdients\n",
    "    dep = sentence['enhancedPlusPlusDependencies']\n",
    "    dep = [{'from':d['governor'],'dep':d['dep'],'to':d['dependent']} for d in dep if d['dep'] not in DEP_ \\\n",
    "       and (d['dependent'],d['governor']) not in span_entity and d['dependentGloss'] not in list_stopWords\\\n",
    "      and d['governorGloss'] not in list_stopWords]# and d['dependentGloss'] in vocab_disasters_and_accdients.keys()\\\n",
    "      #and d['governorGloss'] in vocab_disasters_and_accdients.keys()]\n",
    "    dep_triples =[('/c/en/'+dep_to_str(tokens_dict,entity_dict,d['to']),'/r/tweet/dep/'+d['dep'],\\\n",
    "                   '/c/en/'+dep_to_str(tokens_dict,entity_dict,d['from'])) for d in dep]\n",
    "    # fact_triples filter from dep\n",
    "    fact_dict = defaultdict(dict)\n",
    "    for d in dep:\n",
    "        if d['dep'].startswith('nsubj') or d['dep'].startswith('dobj'):\n",
    "                fact_dict[d['from']].update({d['dep']:d['to']})\n",
    "    fact_triples = []\n",
    "    for k,v in fact_dict.items():\n",
    "        if 'nsubj' in v.keys() and 'dobj' in v.keys():\n",
    "            fact_triples.append(('/c/en/'+dep_to_str(tokens_dict,entity_dict,v['nsubj']),\\\n",
    "                                 '/r/tweet/open/'+dep_to_str(tokens_dict,entity_dict,k),\\\n",
    "                                 '/c/en/'+dep_to_str(tokens_dict,entity_dict,v['dobj'])))\n",
    "    # cat all triples\n",
    "    triples = entity_triples + dep_triples + fact_triples\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_tweet = sample['tweets'][2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_triples = []\n",
    "for index,tweet in enumerate(sample['tweets']):\n",
    "    print(index)\n",
    "    try:\n",
    "        print(tweet['text'])\n",
    "        event_triples.extend(annotate_sentences(tweet['text']))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_triples)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "left = set(event_triples) - set(event_triples_bak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(event_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es = news_event_disasters_and_accdients.take(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4878"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triples = ['a','b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = codecs.open('triples.txt','a+',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.writelines(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
